# Information theory

## Measuring information

* `s` -- # of symbols
* `n` -- message length

minimum amount of information `H` required to transfer a message of length `n` written using `s` symbols is `log(s ^ n)`

	- `s^n` is all the possibilities
	- `log` is binary search over a list of all possible messages of length `n`

or `n * log(s)`:

	- `s` is all symbol possibilities
	- `log` is binary search over s
	- `n` is the binary search repeated `n` times

## Entropy

Shannon notices that most of the information in the world is predictable to a certain degree. He treats this information as something generated by a Markov Process (MP) aka machine. He then defines entropy as an average uncertainty in the MP:

```
H = sum_i p_i * log (1/p_i)
```

	- `p_i` is a probability of a certain symbol `i` being produced by the machine
	- `log(1/p_i)` -- cost of binary search over a search tree for a symbol `i`
	- `H` -- average uncertainty of the whole MP
	
The MP machine can also be visualised as an actual machine, where a pebble is bouncing from top to bottom until a leaf node is reached (symbol). The probability of a symbol defines its position in a search tree. Thus a cost of reaching the symbol can be thought of as a #bounces required for a pebble to reach the node or `log_2(1/p_i)`, thus:

```
H = sum_i * p_i * #bounces
```

Information is actually uncertainty (the more something is uncertain the more information we gain).
